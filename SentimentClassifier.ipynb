{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "tensorflow"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b-ZK9tuGmoK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjqi3WjaGmoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function,division\n",
        "import random \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgQ4CpzZAOW_",
        "colab_type": "code",
        "outputId": "d8a50b38-0f5b-4b0f-a17f-e22a3ca6ea2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "print 'Files in Drive:'\n",
        "!ls drive/\n",
        "\n",
        "# Create a file in Drive.\n",
        "!echo \"This newly created file will appear in your Drive file list.\" > drive/created.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\r\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\r\n",
            "Files in Drive:\n",
            "created.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x32FYgfPazMt",
        "colab_type": "code",
        "outputId": "ed690029-63a5-4430-8560-ea39a0124fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "!ls drive/\n",
        "\n",
        "# Create a file in Drive.\n",
        "!echo \"This newly created file will appear in your Drive file list.\" > drive/created.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A3 (1).ipynb   class_pos.txt\t\t Getting started\r\n",
            "class_neg.txt  Drive_FUSE_example.ipynb\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWPIGJrQCziL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "if sys.version_info >= (3, 0):\n",
        "  from builtins import map as m\n",
        "  def map(f,l):\n",
        "    return list(m(f,l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWzfE86tusct",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Classification - dataset analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atbvZSXlGmoQ",
        "colab_type": "text"
      },
      "source": [
        "We will use movie review dataset taken from http://www.cs.cornell.edu/people/pabo/movie-review-data/. The exact dataset we will use is the Sentence-polarity dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l_qzOlhGmoR",
        "colab_type": "code",
        "outputId": "cb8c6e30-8c57-4b68-c937-bf9eeedeea17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "data = []\n",
        "for file_,label in zip([\"drive/class_neg.txt\",\"drive/class_pos.txt\"],[0,1]):\n",
        "    lines = open(file_).readlines()\n",
        "    lines = list(map(lambda x:x.strip().replace(\"-\",\" \").split(),lines))\n",
        "    for line in lines:\n",
        "        data.append([line,label])\n",
        "    print(\"Number of reviews of {} = {}\".format(file_[:-4],len(lines)))\n",
        "    print(\"\\tMax number of tokens in a sentence = {}\".format(max(map(lambda x:len(x),lines))))\n",
        "    print(\"\\tMin number of tokens in a sentence = {}\".format(min(map(lambda x:len(x),lines))))\n",
        "random.Random(5).shuffle(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews of drive/class_neg = 5331\n",
            "\tMax number of tokens in a sentence = 56\n",
            "\tMin number of tokens in a sentence = 1\n",
            "Number of reviews of drive/class_pos = 5331\n",
            "\tMax number of tokens in a sentence = 59\n",
            "\tMin number of tokens in a sentence = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKqVN0XTGmoV",
        "colab_type": "text"
      },
      "source": [
        "Observe that the lengths of sentences are different. In case, we need to vectorize the operations, we need all sentences to be of equal length. Therefore, we will pad all sentences to be of equal length and substitute the padded parts of sentence with zeros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJSIi3EOGmoV",
        "colab_type": "code",
        "outputId": "90055e47-fa03-4d38-eed7-0f0105ff02f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# See some randomly sampled sentences\n",
        "print(\" \".join(data[random.randint(0,len(data))][0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tries too hard to be funny in a way that's too loud , too goofy and too short of an attention span .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x70n4lA7GmoY",
        "colab_type": "text"
      },
      "source": [
        "We will work with the sentence as given and not remove any stop-words or punctuation marks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "959vht4CGmoZ",
        "colab_type": "code",
        "outputId": "bd0e98bb-646a-4384-de6e-309bfca2c22b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "sents = map(lambda x:x[0],data) # all sentences\n",
        "all_words = set()\n",
        "for sent in sents:\n",
        "    all_words |= set(sent)\n",
        "all_words = sorted(list(all_words))\n",
        "vocab = {all_words[i]:i for i in range(len(all_words))}\n",
        "print(\"Number of words : \",len(vocab))\n",
        "train = data[:int(0.8*len(data))]\n",
        "test = data[int(0.8*len(data)):]\n",
        "train_data = []\n",
        "train_targets = []\n",
        "test_data = []\n",
        "test_targets = []\n",
        "for list_all,list_data,list_target,label_list in zip([train,test],[train_data,test_data],[train_targets,test_targets],[\"train\",\"test\"]):\n",
        "    for datum,label in list_all:\n",
        "        list_data.append([vocab[w] for w in datum])\n",
        "        list_target.append([label])\n",
        "    print(label_list)\n",
        "    print(\"\\tNumber of positive examples : \",list_target.count([1]))\n",
        "    print(\"\\tNumber of negative examples : \",list_target.count([0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words :  19757\n",
            "train\n",
            "\tNumber of positive examples :  4288\n",
            "\tNumber of negative examples :  4241\n",
            "test\n",
            "\tNumber of positive examples :  1043\n",
            "\tNumber of negative examples :  1090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5NZIsmiGmoc",
        "colab_type": "text"
      },
      "source": [
        "For implementation purposes, we will need an index for the padded word and we will use the index 19757.\n",
        "Note: For a dataset of this <i>small</i> size, we will need to do K-Fold Cross-validation to evaluate the performance. However, we will work with this train-test split for the rest of this assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWGatDXGmoc",
        "colab_type": "text"
      },
      "source": [
        "## Simple Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQEg38m6Gmod",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://web.cs.dal.ca/~sastry/cnn_simple.jpg\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCvl-t5EGmoe",
        "colab_type": "text"
      },
      "source": [
        "The above image shows the architecture of the simple model that we will implement for text classification. We are interested in the following hyperparameters apart from the number of filters (which we will set to 1 for this problem):\n",
        "* The span of the filter/the number of words considered for making the prediction.\n",
        "* The size of the stride.\n",
        "* The number of activations selected for feeding into softmax classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrDDcU5GGmoe",
        "colab_type": "text"
      },
      "source": [
        "First, we will write code which can select k top elements in the order they appeared. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SViy9l_Gmog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_max_pool(A,k):\n",
        "    \"\"\"\n",
        "    A = 2 dimensional array (assume that the length of last dimension of A will be always more than k)\n",
        "    k = number of elements.\n",
        "    Return: For every row of A, top k elements in the order they appear.\n",
        "    \"\"\"\n",
        "    assert len(A.get_shape())==2\n",
        "    def func(row):\n",
        "        \"\"\"\n",
        "        Hint : I used top_k and reverse.\n",
        "        I am not sure whether the order of the indices are retained when sorted = False in top_k. (did not find any documentation)\n",
        "        Therefore, I suggest that you sort the indices before selecting the elements from the array(Trick: use top_k again!)\"\"\"\n",
        "        b=tf.nn.top_k(row,k)\n",
        "        c=tf.nn.top_k(b.indices,k)[0]\n",
        "        c=tf.reverse(c,[0])\n",
        "        ret_tensor = tf.gather_nd(row,tf.reshape(c,[k,1]))\n",
        "        ## your code here to compute ret_tensor ##\n",
        "        return ret_tensor\n",
        "    return tf.map_fn(func,A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MayRnDIAGmoi",
        "colab_type": "code",
        "outputId": "14fe367f-42df-4f69-8f57-85169bb161af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "A = tf.placeholder(shape=[None,None],dtype=tf.float64)\n",
        "top = k_max_pool(A,5)\n",
        "sess = tf.Session()\n",
        "for i in range(1,6):\n",
        "    np.random.seed(5)\n",
        "    l = np.random.randn(i*10,i*10)\n",
        "    top_elements = sess.run(top,feed_dict={A:l})\n",
        "    l = l.tolist()\n",
        "    top_elements2 = np.array(map(lambda x: [x[i] for i in range(len(x)) if x[i]>sorted(x,reverse=True)[5]],l))\n",
        "    # Note that this test assumes that the 6th largest element and 5th largest element are different.\n",
        "    print(((top_elements - top_elements2)<10**-10).all())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN66FnNcGmok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initializer(shape):\n",
        "    xavier = tf.contrib.layers.xavier_initializer(seed=1)\n",
        "    return xavier(shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeXmr83YGmom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_simple:\n",
        "    def __init__(self,num_words,embedding_size = 30,span=2,k=5):\n",
        "        self.num_words = num_words\n",
        "\n",
        "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
        "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
        "        self.input = tf.placeholder(shape=[None,100],dtype=tf.int32)\n",
        "        self.expected_output = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
        "        \n",
        "\n",
        "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
        "        # Add an additional row of zeros to denote padded words.\n",
        "        paddings = tf.constant([[0, 1], [0, 0]])\n",
        "        self.embedding_matrix = tf.pad(embedding_matrix, paddings, \"CONSTANT\")\n",
        "        \n",
        "        \n",
        "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
        "        # Use embedding lookup\n",
        "        vectors = tf.nn.embedding_lookup(self.embedding_matrix,self.input) # None x 100 x embedding_size\n",
        "        \n",
        "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
        "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
        "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
        "        # Use expand-dims to modify. \n",
        "        vectors2d = tf.expand_dims(vectors, 1) # None x 1 x 100 x embedding_size\n",
        "        \n",
        "        # Conv2d needs a filter bank.\n",
        "        # The dimensions of the filter bank = Height, Width, in-channels, out-channels(Number-of-Filters).\n",
        "        # We are creating a single filter of size = span. \n",
        "        # So, height = 1, width = span, in-channels = embedding_size ,out-channels = 1. \n",
        "        single_filter = tf.Variable(initializer((1, span, embedding_size, 1)), name=\"filter\")  \n",
        "        bias = tf.Variable(0.0,name=\"bias\") # You need a bias for each filter.\n",
        "        conv_span = tf.nn.conv2d(\n",
        "            input=vectors2d,\n",
        "            filter=single_filter,\n",
        "            # Note that the first and last elements SHOULD be 1. \n",
        "            strides=[1, 1, 1, 1], \n",
        "            # This means that we are ok with input size being reduced during the process of convolution.\n",
        "            padding=\"VALID\"\n",
        "        ) # Shape = None x 1 x 99 x 1\n",
        "        acts = tf.nn.leaky_relu(conv_span+bias)\n",
        "        \n",
        "        # Now, let us extract the top k activations. \n",
        "        # But, we need to first convert acts this into 2-dimensional.  \n",
        "        # Use tf.squeeze. Be sure to specify the squeeze-dimensions\n",
        "        acts_2d = tf.squeeze(acts,[1,3])\n",
        "        \n",
        "        # Use k_max_pool to extract top-k activations\n",
        "        input_fully_connected = k_max_pool(acts_2d,k) # None x k\n",
        "        \n",
        "        # Initialize the weight and bias needed for softmax classifier.\n",
        "        self.softmax_weight = tf.Variable(dtype=tf.float32,initial_value=initializer((k,2)))\n",
        "        self.softmax_bias = tf.Variable(dtype=tf.float32,initial_value=np.zeros(shape=[2]))\n",
        "        \n",
        "        # Write out the equation for computing the logits.\n",
        "        self.output = tf.nn.softmax(tf.matmul(input_fully_connected, self.softmax_weight) + self.softmax_bias, axis=1) # Shape = Nonex2\n",
        "        \n",
        "        # Compute the cross-entropy cost. \n",
        "        # You might either sum or take mean of all the costs across all the examples. \n",
        "        # It is your choice as the test case is on Stochastic Training. \n",
        "        self.cost = tf.reduce_mean(-((self.expected_output*tf.log(self.output[:,1])+((1-self.expected_output)*tf.log(self.output[:,0])))))\n",
        "        \n",
        "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer()\n",
        "        self.train_op = optimizer.minimize(self.cost)\n",
        "        self.session = tf.Session()\n",
        "        self.session.run(tf.global_variables_initializer())\n",
        "\n",
        "    def pad(self,data,pad_word,pad_length=100):\n",
        "        for datum in data:\n",
        "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
        "        return data\n",
        "    \n",
        "    def train(self,train_data,test_data,train_targets,test_targets,batch_size=1,epochs=1,verbose=False):\n",
        "        sess = self.session\n",
        "        self.pad(train_data,self.num_words)\n",
        "        self.pad(test_data,self.num_words)\n",
        "        print(\"Starting training...\")\n",
        "        #print(sess.run(tf.shape(acts)))\n",
        "        for epoch in range(epochs):\n",
        "            cost_epoch = 0\n",
        "            c = 0\n",
        "            for datum,target in zip([train_data[i:i+batch_size] for i in range(0,len(train_data),batch_size)],\n",
        "                                   [train_targets[i:i+batch_size] for i in range(0,len(train_targets),batch_size)]):\n",
        "                _,cost = sess.run([self.train_op,self.cost],feed_dict={self.input:datum,self.expected_output:target})\n",
        "                cost_epoch += cost\n",
        "                c += 1\n",
        "                if c%100 == 0 and verbose:\n",
        "                    print(\"\\t{} batches finished. Cost : {}\".format(c,cost_epoch/c))\n",
        "            print(\"Epoch {}: {}\".format(epoch,cost_epoch/len(train_data)))\n",
        "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data,train_targets)))\n",
        "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data,test_targets)))\n",
        "    \n",
        "    def compute_accuracy(self,data,targets):\n",
        "        return self.session.run(self.accuracy,feed_dict={self.input:data,self.expected_output:targets})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN8RoSLoGmoo",
        "colab_type": "code",
        "outputId": "fadbaab4-2b56-4ef7-c92a-c2dbbf7de606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1512
        }
      },
      "source": [
        "c=CNN_simple(len(vocab))\n",
        "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "\t100 batches finished. Cost : 0.688363182545\n",
            "\t200 batches finished. Cost : 0.695461704433\n",
            "\t300 batches finished. Cost : 0.6959020708\n",
            "\t400 batches finished. Cost : 0.697339072824\n",
            "\t500 batches finished. Cost : 0.698220447183\n",
            "\t600 batches finished. Cost : 0.697462852498\n",
            "\t700 batches finished. Cost : 0.697759581293\n",
            "\t800 batches finished. Cost : 0.697326156646\n",
            "\t900 batches finished. Cost : 0.696654122339\n",
            "\t1000 batches finished. Cost : 0.696934172928\n",
            "\t1100 batches finished. Cost : 0.696292648017\n",
            "\t1200 batches finished. Cost : 0.695106213714\n",
            "\t1300 batches finished. Cost : 0.695746991932\n",
            "\t1400 batches finished. Cost : 0.69638582832\n",
            "\t1500 batches finished. Cost : 0.69563915801\n",
            "\t1600 batches finished. Cost : 0.695392685775\n",
            "\t1700 batches finished. Cost : 0.695754958128\n",
            "\t1800 batches finished. Cost : 0.695977654043\n",
            "\t1900 batches finished. Cost : 0.695701952247\n",
            "\t2000 batches finished. Cost : 0.695247689158\n",
            "\t2100 batches finished. Cost : 0.694428743805\n",
            "\t2200 batches finished. Cost : 0.694853385822\n",
            "\t2300 batches finished. Cost : 0.694558180065\n",
            "\t2400 batches finished. Cost : 0.694085539517\n",
            "\t2500 batches finished. Cost : 0.693020987403\n",
            "\t2600 batches finished. Cost : 0.692662144601\n",
            "\t2700 batches finished. Cost : 0.693177674258\n",
            "\t2800 batches finished. Cost : 0.692366178408\n",
            "\t2900 batches finished. Cost : 0.691678478944\n",
            "\t3000 batches finished. Cost : 0.691966544429\n",
            "\t3100 batches finished. Cost : 0.692257468249\n",
            "\t3200 batches finished. Cost : 0.691529861186\n",
            "\t3300 batches finished. Cost : 0.691115460459\n",
            "\t3400 batches finished. Cost : 0.691398814654\n",
            "\t3500 batches finished. Cost : 0.691416790213\n",
            "\t3600 batches finished. Cost : 0.691391663171\n",
            "\t3700 batches finished. Cost : 0.691068919744\n",
            "\t3800 batches finished. Cost : 0.690949399056\n",
            "\t3900 batches finished. Cost : 0.690517386955\n",
            "\t4000 batches finished. Cost : 0.690368138075\n",
            "\t4100 batches finished. Cost : 0.690143618337\n",
            "\t4200 batches finished. Cost : 0.690339895586\n",
            "\t4300 batches finished. Cost : 0.690389285753\n",
            "\t4400 batches finished. Cost : 0.690553119271\n",
            "\t4500 batches finished. Cost : 0.690163690448\n",
            "\t4600 batches finished. Cost : 0.689811406149\n",
            "\t4700 batches finished. Cost : 0.689431954927\n",
            "\t4800 batches finished. Cost : 0.689024882646\n",
            "\t4900 batches finished. Cost : 0.688819048946\n",
            "\t5000 batches finished. Cost : 0.688381816149\n",
            "\t5100 batches finished. Cost : 0.688175238026\n",
            "\t5200 batches finished. Cost : 0.688049951797\n",
            "\t5300 batches finished. Cost : 0.687438222175\n",
            "\t5400 batches finished. Cost : 0.687262039941\n",
            "\t5500 batches finished. Cost : 0.68719595558\n",
            "\t5600 batches finished. Cost : 0.686344017703\n",
            "\t5700 batches finished. Cost : 0.686179797304\n",
            "\t5800 batches finished. Cost : 0.685655907272\n",
            "\t5900 batches finished. Cost : 0.685972156856\n",
            "\t6000 batches finished. Cost : 0.685690109474\n",
            "\t6100 batches finished. Cost : 0.685392786496\n",
            "\t6200 batches finished. Cost : 0.684812556825\n",
            "\t6300 batches finished. Cost : 0.684353355709\n",
            "\t6400 batches finished. Cost : 0.684047549167\n",
            "\t6500 batches finished. Cost : 0.683380806776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\t6600 batches finished. Cost : 0.683325836033\n",
            "\t6700 batches finished. Cost : 0.682583970579\n",
            "\t6800 batches finished. Cost : 0.681868460882\n",
            "\t6900 batches finished. Cost : 0.682024431525\n",
            "\t7000 batches finished. Cost : 0.681632122029\n",
            "\t7100 batches finished. Cost : 0.680960628168\n",
            "\t7200 batches finished. Cost : 0.680770801177\n",
            "\t7300 batches finished. Cost : 0.680385479517\n",
            "\t7400 batches finished. Cost : 0.679857558429\n",
            "\t7500 batches finished. Cost : 0.679272243466\n",
            "\t7600 batches finished. Cost : 0.678875331777\n",
            "\t7700 batches finished. Cost : 0.67837705911\n",
            "\t7800 batches finished. Cost : 0.678056778775\n",
            "\t7900 batches finished. Cost : 0.678070614286\n",
            "\t8000 batches finished. Cost : 0.677414160635\n",
            "\t8100 batches finished. Cost : 0.676894019406\n",
            "\t8200 batches finished. Cost : 0.676402020847\n",
            "\t8300 batches finished. Cost : 0.675500193958\n",
            "\t8400 batches finished. Cost : 0.674959771196\n",
            "\t8500 batches finished. Cost : 0.67482759468\n",
            "Epoch 0: 0.675099702244\n",
            "\tTrain accuracy: 0.718958854675\n",
            "\tTest accuracy: 0.664322555065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qAcWbzyGmop",
        "colab_type": "text"
      },
      "source": [
        "The expected output for the above snippet is\n",
        "<pre>\n",
        "Starting training...\n",
        "\t100 batches finished. Cost : 0.688363179564\n",
        "\t200 batches finished. Cost : 0.695461705327\n",
        "\t300 batches finished. Cost : 0.695902070602\n",
        "\t400 batches finished. Cost : 0.697339072227\n",
        "\t500 batches finished. Cost : 0.698220448136\n",
        "    ...\n",
        "Epoch 0: 0.675099702418\n",
        "\tTrain accuracy: 0.718958854675\n",
        "\tTest accuracy: 0.664322555065   \n",
        "</pre>\n",
        "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtV-0gInGmoq",
        "colab_type": "text"
      },
      "source": [
        "## ConvNet "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi_un8GvGmor",
        "colab_type": "text"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrPWOepGmor",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://web.cs.dal.ca/~sastry/cnn.png\" style=\"height:40%;width:40%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLMzzdMBGmos",
        "colab_type": "text"
      },
      "source": [
        "Essentially, there are 2 kind of hyper-parameters - the filter size and number of filters of each size. In the image shown, there are 3 filter-sizes - 2,3,4 and number of filters of each size is 2. Once the convolution is obtained, 1-max pooling is done - it basically involves extracting 1 activation from the list of activations which is the maximum activation. The reason we need to do this is to construct the inputs to the softmax layer which are of a fixed size.\n",
        "Read more at https://arxiv.org/pdf/1510.03820.pdf. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqoRWB9wGmot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN:\n",
        "    def __init__(self,num_words,embedding_size = 30):\n",
        "        self.num_words = num_words\n",
        "\n",
        "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
        "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
        "        self.input = tf.placeholder(shape=[None,100],dtype=tf.int32)\n",
        "        self.expected_output = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
        "        \n",
        "\n",
        "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
        "        # Add an additional row of zeros to denote padded words.\n",
        "        paddings = tf.constant([[0, 1], [0, 0]])\n",
        "        self.embedding_matrix = tf.pad(embedding_matrix, paddings, \"CONSTANT\")\n",
        "        \n",
        "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
        "        # Use embedding lookup\n",
        "        vectors = tf.nn.embedding_lookup(self.embedding_matrix,self.input) # None x 100 x embedding_size\n",
        "        \n",
        "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
        "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
        "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
        "        # Use expand-dims to modify. \n",
        "        vectors2d = tf.expand_dims(vectors, 1) # None x 1 x 100 x embedding_size\n",
        "        \n",
        "        # Create 50 filters with span of 3 words. You need 1 bias for each filter.\n",
        "        filter_tri = tf.Variable(initializer((1,3,embedding_size,50)), name=\"weight3\")  \n",
        "        bias_tri = tf.Variable(tf.zeros((1,50)), name=\"bias3\")  \n",
        "        conv1 = tf.nn.conv2d(\n",
        "            input=vectors2d,\n",
        "            filter=filter_tri,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"VALID\"\n",
        "        )  # Shape = ?\n",
        "        A1 = tf.nn.leaky_relu(conv1+bias_tri)\n",
        "\n",
        "        # Create 50 filters with span of 4 words. You need 1 bias for each filter.\n",
        "        filter_4 = tf.Variable(initializer((1,4,embedding_size,50)), name=\"weight4\")  \n",
        "        bias_4 = tf.Variable(tf.zeros((1,50)), name=\"bias4\")\n",
        "        conv2 = tf.nn.conv2d(\n",
        "            input=vectors2d,\n",
        "            filter=filter_4,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"VALID\"\n",
        "        )  # Shape = ?\n",
        "\n",
        "        A2 = tf.nn.leaky_relu(conv2+bias_4)\n",
        "\n",
        "        # Create 50 filters with span of 5 words. You need 1 bias for each filter.\n",
        "        filter_5 = tf.Variable(initializer((1,5,embedding_size,50)), name=\"weight5\")  \n",
        "        bias_5 = tf.Variable(tf.zeros((1,50)), name=\"bias5\")\n",
        "        conv3 = tf.nn.conv2d(\n",
        "            input=vectors2d,\n",
        "            filter=filter_5,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"VALID\"\n",
        "        )  # Shape = ?\n",
        "\n",
        "        A3 = tf.nn.leaky_relu(conv3+bias_5)\n",
        "        \n",
        "        A1_2d = tf.squeeze(A1,[1])\n",
        "        A2_2d = tf.squeeze(A2,[1])\n",
        "        A3_2d = tf.squeeze(A3,[1])\n",
        "        \n",
        "        # Now extract the maximum activations for each of the filters. The shapes are listed alongside. \n",
        "        max_A1 = tf.reduce_max(A1_2d,reduction_indices=[1])  # None x 50\n",
        "        max_A2 =  tf.reduce_max(A2_2d,reduction_indices=[1])  # None x 50\n",
        "        max_A3 =  tf.reduce_max(A3_2d,reduction_indices=[1])  # None x 50\n",
        "        \n",
        "        concat = tf.concat([max_A1, max_A2, max_A3], axis=1) # None x 150\n",
        "        \n",
        "        # Initialize the weight and bias needed for softmax classifier. \n",
        "        self.softmax_weight = tf.Variable(dtype=tf.float32,initial_value=initializer((150,2)))\n",
        "        self.softmax_bias = tf.Variable(dtype=tf.float32,initial_value=np.zeros(shape=[2]))\n",
        "        \n",
        "        # Write out the equation for computing the logits.\n",
        "        self.output = tf.nn.softmax(tf.matmul(concat, self.softmax_weight) + self.softmax_bias, axis=1) # Shape = ?\n",
        "        \n",
        "        # Compute the cross-entropy cost. \n",
        "        # You might either sum or take mean of all the costs across all the examples. \n",
        "        # It is your choice as the test case is on Stochastic Training. \n",
        "        self.cost = tf.reduce_mean(-((self.expected_output*tf.log(self.output[:,1])+((1-self.expected_output)*tf.log(self.output[:,0])))))\n",
        "        \n",
        "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer()\n",
        "        self.train_op = optimizer.minimize(self.cost)\n",
        "        self.session = tf.Session()\n",
        "        self.session.run(tf.global_variables_initializer())\n",
        "\n",
        "    def pad(self,data,pad_word,pad_length=100):\n",
        "        for datum in data:\n",
        "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
        "        return data\n",
        "    \n",
        "    def train(self,train_data,test_data,train_targets,test_targets,batch_size=1,epochs=1,verbose=False):\n",
        "        sess = self.session\n",
        "        self.pad(train_data,self.num_words)\n",
        "        self.pad(test_data,self.num_words)\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(epochs):\n",
        "            cost_epoch = 0\n",
        "            c = 0\n",
        "            for datum,target in zip([train_data[i:i+batch_size] for i in range(0,len(train_data),batch_size)],\n",
        "                                   [train_targets[i:i+batch_size] for i in range(0,len(train_targets),batch_size)]):\n",
        "                _,cost = sess.run([self.train_op,self.cost],feed_dict={self.input:datum,self.expected_output:target})\n",
        "                cost_epoch += cost\n",
        "                c += 1\n",
        "                if c%100 == 0 and verbose:\n",
        "                    print(\"\\t{} batches finished. Cost : {}\".format(c,cost_epoch/c))\n",
        "            print(\"Epoch {}: {}\".format(epoch,cost_epoch/len(train_data)))\n",
        "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data,train_targets)))\n",
        "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data,test_targets)))\n",
        "    \n",
        "    def compute_accuracy(self,data,targets):\n",
        "        return self.session.run(self.accuracy,feed_dict={self.input:data,self.expected_output:targets})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfYIkXOkGmov",
        "colab_type": "code",
        "outputId": "4d63d72d-3e85-4b74-bdfa-957ad1b7a7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1512
        }
      },
      "source": [
        "c=CNN(len(vocab))\n",
        "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "\t100 batches finished. Cost : 0.692921407223\n",
            "\t200 batches finished. Cost : 0.694593489766\n",
            "\t300 batches finished. Cost : 0.69501670599\n",
            "\t400 batches finished. Cost : 0.695035843849\n",
            "\t500 batches finished. Cost : 0.693291912556\n",
            "\t600 batches finished. Cost : 0.692537607749\n",
            "\t700 batches finished. Cost : 0.692302573579\n",
            "\t800 batches finished. Cost : 0.688138688877\n",
            "\t900 batches finished. Cost : 0.684975062774\n",
            "\t1000 batches finished. Cost : 0.673650282636\n",
            "\t1100 batches finished. Cost : 0.66513232009\n",
            "\t1200 batches finished. Cost : 0.658742382556\n",
            "\t1300 batches finished. Cost : 0.657242691471\n",
            "\t1400 batches finished. Cost : 0.659336286146\n",
            "\t1500 batches finished. Cost : 0.656265231178\n",
            "\t1600 batches finished. Cost : 0.654087435502\n",
            "\t1700 batches finished. Cost : 0.652741391861\n",
            "\t1800 batches finished. Cost : 0.65330711311\n",
            "\t1900 batches finished. Cost : 0.651790104513\n",
            "\t2000 batches finished. Cost : 0.649372568377\n",
            "\t2100 batches finished. Cost : 0.64534604844\n",
            "\t2200 batches finished. Cost : 0.645869592195\n",
            "\t2300 batches finished. Cost : 0.642412474429\n",
            "\t2400 batches finished. Cost : 0.640614605907\n",
            "\t2500 batches finished. Cost : 0.638334024004\n",
            "\t2600 batches finished. Cost : 0.636367115989\n",
            "\t2700 batches finished. Cost : 0.635102530122\n",
            "\t2800 batches finished. Cost : 0.636930496425\n",
            "\t2900 batches finished. Cost : 0.63316704299\n",
            "\t3000 batches finished. Cost : 0.632503435006\n",
            "\t3100 batches finished. Cost : 0.631279619131\n",
            "\t3200 batches finished. Cost : 0.629251734092\n",
            "\t3300 batches finished. Cost : 0.629185047878\n",
            "\t3400 batches finished. Cost : 0.627117735459\n",
            "\t3500 batches finished. Cost : 0.627542999322\n",
            "\t3600 batches finished. Cost : 0.625731636404\n",
            "\t3700 batches finished. Cost : 0.624322643107\n",
            "\t3800 batches finished. Cost : 0.623208081806\n",
            "\t3900 batches finished. Cost : 0.620638983292\n",
            "\t4000 batches finished. Cost : 0.617501062292\n",
            "\t4100 batches finished. Cost : 0.615965228463\n",
            "\t4200 batches finished. Cost : 0.613940786085\n",
            "\t4300 batches finished. Cost : 0.610867118951\n",
            "\t4400 batches finished. Cost : 0.611552019221\n",
            "\t4500 batches finished. Cost : 0.610347683752\n",
            "\t4600 batches finished. Cost : 0.610923199072\n",
            "\t4700 batches finished. Cost : 0.610161990732\n",
            "\t4800 batches finished. Cost : 0.608298673222\n",
            "\t4900 batches finished. Cost : 0.606209454055\n",
            "\t5000 batches finished. Cost : 0.606260402352\n",
            "\t5100 batches finished. Cost : 0.604698692755\n",
            "\t5200 batches finished. Cost : 0.603607866453\n",
            "\t5300 batches finished. Cost : 0.602729674424\n",
            "\t5400 batches finished. Cost : 0.602036698178\n",
            "\t5500 batches finished. Cost : 0.600645291618\n",
            "\t5600 batches finished. Cost : 0.599811554488\n",
            "\t5700 batches finished. Cost : 0.598752141332\n",
            "\t5800 batches finished. Cost : 0.597146166071\n",
            "\t5900 batches finished. Cost : 0.595692980133\n",
            "\t6000 batches finished. Cost : 0.595275652182\n",
            "\t6100 batches finished. Cost : 0.594847665442\n",
            "\t6200 batches finished. Cost : 0.593510831641\n",
            "\t6300 batches finished. Cost : 0.593101585457\n",
            "\t6400 batches finished. Cost : 0.591802941269\n",
            "\t6500 batches finished. Cost : 0.59056559309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\t6600 batches finished. Cost : 0.588505509759\n",
            "\t6700 batches finished. Cost : 0.58700422433\n",
            "\t6800 batches finished. Cost : 0.586377448646\n",
            "\t6900 batches finished. Cost : 0.586005952792\n",
            "\t7000 batches finished. Cost : 0.58512639721\n",
            "\t7100 batches finished. Cost : 0.583860160822\n",
            "\t7200 batches finished. Cost : 0.5825608631\n",
            "\t7300 batches finished. Cost : 0.582344473591\n",
            "\t7400 batches finished. Cost : 0.581754032691\n",
            "\t7500 batches finished. Cost : 0.580579865641\n",
            "\t7600 batches finished. Cost : 0.579680734467\n",
            "\t7700 batches finished. Cost : 0.579085824643\n",
            "\t7800 batches finished. Cost : 0.578347276748\n",
            "\t7900 batches finished. Cost : 0.577519263627\n",
            "\t8000 batches finished. Cost : 0.578204833062\n",
            "\t8100 batches finished. Cost : 0.577526843726\n",
            "\t8200 batches finished. Cost : 0.577229891062\n",
            "\t8300 batches finished. Cost : 0.575433941908\n",
            "\t8400 batches finished. Cost : 0.573412873759\n",
            "\t8500 batches finished. Cost : 0.572945575809\n",
            "Epoch 0: 0.5740312021\n",
            "\tTrain accuracy: 0.894360423088\n",
            "\tTest accuracy: 0.761837780476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhrJL02kGmow",
        "colab_type": "text"
      },
      "source": [
        "The expected output for the above snippet is\n",
        "<pre>\n",
        "Starting training...\n",
        "\t100 batches finished. Cost : 0.692921404839\n",
        "\t200 batches finished. Cost : 0.694593518078\n",
        "\t300 batches finished. Cost : 0.695016788642\n",
        "\t400 batches finished. Cost : 0.695038306713\n",
        "\t500 batches finished. Cost : 0.693231915712\n",
        "    ...\n",
        "Epoch 0: 0.571991487547\n",
        "\tTrain accuracy: 0.895532906055\n",
        "\tTest accuracy: 0.759962499142 \n",
        "</pre>\n",
        "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ozMFo7kGmow",
        "colab_type": "text"
      },
      "source": [
        "### Effect of Batch Size on Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tybsvV02Gmox",
        "colab_type": "text"
      },
      "source": [
        "Study the effects of changing batch size. Just run the various experiments and observe the results (Run it in non-verbose mode). No need to make any comments here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JjNaWPtGmoy",
        "colab_type": "code",
        "outputId": "222ac4f6-c5b6-48c2-f62d-6de6f1ff7cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "c=CNN(len(vocab))\n",
        "c.train(train_data,test_data,train_targets,test_targets,4,epochs=1,verbose=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 0: 0.173166305431\n",
            "\tTrain accuracy: 0.719662308693\n",
            "\tTest accuracy: 0.679793715477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEJIvUrTVmB",
        "colab_type": "code",
        "outputId": "7e79f436-3def-4482-a279-ae940c35f14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "c=CNN(len(vocab))\n",
        "c.train(train_data,test_data,train_targets,test_targets,6,epochs=1,verbose=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 0: 0.115678352248\n",
            "\tTrain accuracy: 0.689998805523\n",
            "\tTest accuracy: 0.661040782928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01QMr12uTxLa",
        "colab_type": "code",
        "outputId": "a145e2b9-f5c2-4648-cbfc-138886999833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "c=CNN(len(vocab))\n",
        "c.train(train_data,test_data,train_targets,test_targets,10,epochs=1,verbose=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 0: 0.0693999522176\n",
            "\tTrain accuracy: 0.649431347847\n",
            "\tTest accuracy: 0.639006078243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPcVkOYgUxs6",
        "colab_type": "code",
        "outputId": "c8238139-2bb5-4107-c7b3-47bfd5cbb919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "c=CNN(len(vocab))\n",
        "c.train(train_data,test_data,train_targets,test_targets,20,epochs=1,verbose=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 0: 0.0347383128533\n",
            "\tTrain accuracy: 0.570055127144\n",
            "\tTest accuracy: 0.541959702969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzOv1TG_Gmoz",
        "colab_type": "text"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVha1w_BGmo0",
        "colab_type": "text"
      },
      "source": [
        "2 functions - get_distance and get_most_similar to the CNN class (the big one). \n",
        "* get_distance(word1,word2) - should return the cosine distance between the 2 words.\n",
        "* get_most_similar(word) - should return top 10 most similar words to the word passed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw4bN713Gmo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN2:\n",
        "    def __init__(self,num_words,embedding_size = 30):\n",
        "        self.num_words = num_words\n",
        "        self.embedding_size = embedding_size\n",
        "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
        "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
        "        self.input = tf.placeholder(shape=[None,100],dtype=tf.int32)\n",
        "        self.expected_output = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
        "        \n",
        "\n",
        "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
        "        # Add an additional row of zeros to denote padded words.\n",
        "        paddings = tf.constant([[0, 1], [0, 0]])\n",
        "        self.embedding_matrix = tf.pad(embedding_matrix, paddings, \"CONSTANT\")\n",
        "        \n",
        "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
        "        # Use embedding lookup\n",
        "        vectors = tf.nn.embedding_lookup(self.embedding_matrix,self.input) # None x 100 x embedding_size\n",
        "        \n",
        "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
        "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
        "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
        "        # Use expand-dims to modify. \n",
        "        vectors2d = tf.expand_dims(vectors, 1) # None x 1 x 100 x embedding_size\n",
        "        \n",
        "        # Create 50 filters with span of 3 words. You need 1 bias for each filter.\n",
        "        filter_tri = tf.Variable(initializer((1,3,embedding_size,50)), name=\"weight3\")  \n",
        "        bias_tri = tf.Variable(tf.zeros((1,50)), name=\"bias3\")  \n",
        "        conv1 = tf.nn.conv2d(\n",
        "            input=vectors2d,\n",
        "            filter=filter_tri,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"VALID\"\n",
        "        )  # Shape = ?\n",
        "        A1 = tf.nn.leaky_relu(conv1+bias_tri)\n",
        "\n",
        "        # Create 50 filters with span of 4 words. You need 1 bias for each filter.\n",
        "        filter_4 = tf.Variable(initializer((1,4,embedding_size,50)), name=\"weight4\")  \n",
        "        bias_4 = tf.Variable(tf.zeros((1,50)), name=\"bias4\")\n",
        "        conv2 = tf.nn.conv2d(\n",
        "            input=vectors2d,\n",
        "            filter=filter_4,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"VALID\"\n",
        "        )  # Shape = ?\n",
        "\n",
        "        A2 = tf.nn.leaky_relu(conv2+bias_4)\n",
        "\n",
        "        # Create 50 filters with span of 5 words. You need 1 bias for each filter.\n",
        "        filter_5 = tf.Variable(initializer((1,5,embedding_size,50)), name=\"weight5\")  \n",
        "        bias_5 = tf.Variable(tf.zeros((1,50)), name=\"bias5\")\n",
        "        conv3 = tf.nn.conv2d(\n",
        "            input=vectors2d,\n",
        "            filter=filter_5,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"VALID\"\n",
        "        )  # Shape = ?\n",
        "\n",
        "        A3 = tf.nn.leaky_relu(conv3+bias_5)\n",
        "        \n",
        "        A1_2d = tf.squeeze(A1,[1])\n",
        "        A2_2d = tf.squeeze(A2,[1])\n",
        "        A3_2d = tf.squeeze(A3,[1])\n",
        "        \n",
        "        # Now extract the maximum activations for each of the filters. The shapes are listed alongside. \n",
        "        max_A1 = tf.reduce_max(A1_2d,reduction_indices=[1])  # None x 50\n",
        "        max_A2 =  tf.reduce_max(A2_2d,reduction_indices=[1])  # None x 50\n",
        "        max_A3 =  tf.reduce_max(A3_2d,reduction_indices=[1])  # None x 50\n",
        "        \n",
        "        concat = tf.concat([max_A1, max_A2, max_A3], axis=1) # None x 150\n",
        "        \n",
        "        # Initialize the weight and bias needed for softmax classifier. \n",
        "        self.softmax_weight = tf.Variable(dtype=tf.float32,initial_value=initializer((150,2)))\n",
        "        self.softmax_bias = tf.Variable(dtype=tf.float32,initial_value=np.zeros(shape=[2]))\n",
        "        \n",
        "        # Write out the equation for computing the logits.\n",
        "        self.output = tf.nn.softmax(tf.matmul(concat, self.softmax_weight) + self.softmax_bias, axis=1) # Shape = ?\n",
        "        \n",
        "        # Compute the cross-entropy cost. \n",
        "        # You might either sum or take mean of all the costs across all the examples. \n",
        "        # It is your choice as the test case is on Stochastic Training. \n",
        "        self.cost = tf.reduce_mean(-((self.expected_output*tf.log(self.output[:,1])+((1-self.expected_output)*tf.log(self.output[:,0])))))\n",
        "        \n",
        "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer()\n",
        "        self.train_op = optimizer.minimize(self.cost)\n",
        "        self.session = tf.Session()\n",
        "        self.session.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def get_distance(self,word1,word2):\n",
        "      sess1=self.session\n",
        "      #First let's load meta graph and restore weights\n",
        "      saver = tf.train.import_meta_graph('drive/my_cnn_model.meta')\n",
        "      saver.restore(sess1,tf.train.latest_checkpoint('drive/./'))\n",
        "\n",
        "      graph = tf.get_default_graph()\n",
        "      embedding_matrix = graph.get_tensor_by_name(\"self.embedding_matrix:0\")\n",
        "\n",
        "      index1=vocab[word1]\n",
        "      index2=vocab[word2]\n",
        "      embedding1=embedding_matrix[index1,:]\n",
        "      embedding2=embedding_matrix[index2,:]\n",
        "      l1=tf.nn.l2_normalize(embedding1,axis=0)\n",
        "      l2=tf.nn.l2_normalize(embedding2,axis=0)\n",
        "      distance=tf.reduce_sum(tf.multiply(l1,l2))\n",
        "      return self.session.run(distance)\n",
        "    \n",
        "    def get_most_similar_indices(self,word):\n",
        "      length=len(vocab)+1\n",
        "      t=tf.nn.l2_normalize(self.embedding_matrix,axis=1)\n",
        "      index2=vocab[word]\n",
        "      ts=self.embedding_matrix[index2,:]\n",
        "      p=tf.nn.l2_normalize(ts,axis=0)\n",
        "      p=tf.reshape(p,[1,self.embedding_size])\n",
        "      p=tf.tile(p,[length,1])\n",
        "      #s = 1-tf.losses.cosine_distance(p, t, dim=0)\n",
        "      s=1-tf.reduce_sum(tf.multiply(p,t),reduction_indices=[1])\n",
        "      s=tf.reshape(s,[1,length])\n",
        "      r=tf.nn.top_k(s,11).indices\n",
        "      return r\n",
        "    \n",
        "    def get_most_similar(self,word):\n",
        "      sess2 = self.session\n",
        "      #sess2.run(tf.global_variables_initializer())\n",
        "      #print(sess.run(tf.cast(k_max_pool(s,11),tf.int32)))\n",
        "      tp=sess2.run(self.get_most_similar_indices(word))\n",
        "      words=[]\n",
        "      for i in tp[0,:]:\n",
        "          words.append(all_words[i])\n",
        "      return words\n",
        "\n",
        "    def pad(self,data,pad_word,pad_length=100):\n",
        "        for datum in data:\n",
        "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
        "        return data\n",
        "    \n",
        "    def train(self,train_data,test_data,train_targets,test_targets,batch_size=1,epochs=1,verbose=False):\n",
        "        sess = self.session\n",
        "        self.pad(train_data,self.num_words)\n",
        "        self.pad(test_data,self.num_words)\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(epochs):\n",
        "            cost_epoch = 0\n",
        "            c = 0\n",
        "            for datum,target in zip([train_data[i:i+batch_size] for i in range(0,len(train_data),batch_size)],\n",
        "                                   [train_targets[i:i+batch_size] for i in range(0,len(train_targets),batch_size)]):\n",
        "                saver = tf.train.Saver()\n",
        "                _,cost = sess.run([self.train_op,self.cost],feed_dict={self.input:datum,self.expected_output:target})\n",
        "                cost_epoch += cost\n",
        "                c += 1\n",
        "                if c%5==0:\n",
        "\n",
        "                  #Run the operation by feeding input\n",
        "                  #Prints 24 which is sum of (w1+w2)*b1 \n",
        "\n",
        "                  #Now, save the graph\n",
        "                  saver.save(sess, 'drive/my_cnn_model')\n",
        "                  print(self.get_distance('good','bad'))\n",
        "                if c%100 == 0 and verbose:\n",
        "                    print(\"\\t{} batches finished. Cost : {}\".format(c,cost_epoch/c))\n",
        "            print(\"Epoch {}: {}\".format(epoch,cost_epoch/len(train_data)))\n",
        "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data,train_targets)))\n",
        "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data,test_targets)))\n",
        "    \n",
        "    def compute_accuracy(self,data,targets):\n",
        "        return self.session.run(self.accuracy,feed_dict={self.input:data,self.expected_output:targets})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMOB3iO5ag_X",
        "colab_type": "text"
      },
      "source": [
        "### Learnings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj0s5x-FarR3",
        "colab_type": "text"
      },
      "source": [
        "List out the observations and conclusions you made from the various experiments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpC2Sjpfa4Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}